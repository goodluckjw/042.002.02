import requests
import xml.etree.ElementTree as ET
from urllib.parse import quote
import re
import os

OC = os.getenv("OC", "chetera")
BASE = "http://www.law.go.kr"

def get_law_list_from_api(query):
    exact_query = f'\"{query}\"'
    encoded_query = quote(f'"{query}"')  # âœ… í•µì‹¬ ìˆ˜ì • í¬ì¸íŠ¸
    page = 1
    laws = []
    while True:
        url = f"{BASE}/DRF/lawSearch.do?OC={OC}&target=law&type=XML&display=100&page={page}&search=2&knd=A0002&query={encoded_query}"
        res = requests.get(url, timeout=10)
        res.encoding = 'utf-8'
        if res.status_code != 200:
            break
        root = ET.fromstring(res.content)
        for law in root.findall("law"):
            laws.append({
                "ë²•ë ¹ëª…": law.findtext("ë²•ë ¹ëª…í•œê¸€", "").strip(),
                "MST": law.findtext("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸", ""),
            })
        if len(root.findall("law")) < 100:
            break
        page += 1
    return laws

def get_law_text_by_mst(mst):
    url = f"{BASE}/DRF/lawService.do?OC={OC}&target=law&MST={mst}&type=XML"
    try:
        res = requests.get(url, timeout=10)
        res.encoding = 'utf-8'
        return res.content if res.status_code == 200 else None
    except:
        return None

def clean(text):
    return re.sub(r"\s+", "", text or "")

def extract_locations(xml_data, keyword):
    tree = ET.fromstring(xml_data)
    articles = tree.findall(".//ì¡°ë¬¸ë‹¨ìœ„")
    keyword_clean = clean(keyword)
    locations = []

    for article in articles:
        ì¡°ë²ˆí˜¸ = article.findtext("ì¡°ë²ˆí˜¸", "").strip()
        ì¡°ì œëª© = article.findtext("ì¡°ë¬¸ì œëª©", "") or ""
        ì¡°ë‚´ìš© = article.findtext("ì¡°ë¬¸ë‚´ìš©", "") or ""
        í•­ë“¤ = article.findall("í•­")

        if keyword_clean in clean(ì¡°ì œëª©):
            locations.append(f"ì œ{ì¡°ë²ˆí˜¸}ì¡°ì˜ ì œëª©")
        if keyword_clean in clean(ì¡°ë‚´ìš©):
            locations.append(f"ì œ{ì¡°ë²ˆí˜¸}ì¡°")

        for í•­ in í•­ë“¤:
            í•­ë²ˆí˜¸ = í•­.findtext("í•­ë²ˆí˜¸", "").strip()
            í•­ë‚´ìš© = í•­.findtext("í•­ë‚´ìš©", "") or ""
            if keyword_clean in clean(í•­ë‚´ìš©):
                locations.append(f"ì œ{ì¡°ë²ˆí˜¸}ì¡°ì œ{í•­ë²ˆí˜¸}í•­")

    return locations

def deduplicate(seq):
    seen = set()
    return [x for x in seq if not (x in seen or seen.add(x))]

def format_location_list(locations):
    if not locations:
        return ""
    if len(locations) == 1:
        return locations[0]
    else:
        return " ë° ".join(locations[:-1]) + " ë° " + locations[-1]

def get_josa(word, josa_with_batchim, josa_without_batchim):
    if not word:
        return josa_with_batchim
    last_char = word[-1]
    code = ord(last_char)
    return josa_with_batchim if (code - 44032) % 28 != 0 else josa_without_batchim

def run_amendment_logic(find_word, replace_word):
    ì¡°ì‚¬ = get_josa(find_word, "ì„", "ë¥¼")
    amendment_results = []

    for law in get_law_list_from_api(find_word):
        law_name = law["ë²•ë ¹ëª…"]
        mst = law["MST"]
        xml = get_law_text_by_mst(mst)
        if not xml:
            continue
        locations = extract_locations(xml, find_word)
        if not locations:
            continue
        unique_locs = deduplicate(locations)
        loc_str = format_location_list(unique_locs)
        sentence = f"â‘  {law_name} ì¼ë¶€ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°œì •í•œë‹¤. {loc_str} ì¤‘ â€œ{find_word}â€{ì¡°ì‚¬} ê°ê° â€œ{replace_word}â€ë¡œ í•œë‹¤."
        amendment_results.append(sentence)

    return amendment_results if amendment_results else ["âš ï¸ ê°œì • ëŒ€ìƒ ì¡°ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."]

    print("ğŸ“Œ ìµœì‹  law_processor.py ë¡œì§ì´ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
